{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: numpy in /home/emr-notebook/.local/lib/python3.7/site-packages (1.21.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: matplotlib in /home/emr-notebook/.local/lib/python3.7/site-packages (3.4.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/emr-notebook/.local/lib/python3.7/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /mnt/notebook-env/lib/python3.7/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/emr-notebook/.local/lib/python3.7/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from matplotlib) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/emr-notebook/.local/lib/python3.7/site-packages (from matplotlib) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/notebook-env/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/notebook-env/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.15.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: joblib in /home/emr-notebook/.local/lib/python3.7/site-packages (1.1.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: pyspark==3.1.1 in /home/emr-notebook/.local/lib/python3.7/site-packages (3.1.1)\n",
      "Requirement already satisfied: py4j==0.10.9 in /home/emr-notebook/.local/lib/python3.7/site-packages (from pyspark==3.1.1) (0.10.9)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: sklearn in /home/emr-notebook/.local/lib/python3.7/site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /home/emr-notebook/.local/lib/python3.7/site-packages (from sklearn) (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: scikit-learn in /home/emr-notebook/.local/lib/python3.7/site-packages (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn) (1.21.5)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn) (1.7.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn) (3.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: lightgbm in /home/emr-notebook/.local/lib/python3.7/site-packages (3.3.1)\n",
      "Requirement already satisfied: wheel in /mnt/notebook-env/lib/python3.7/site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: numpy in /home/emr-notebook/.local/lib/python3.7/site-packages (from lightgbm) (1.21.5)\n",
      "Requirement already satisfied: scipy in /home/emr-notebook/.local/lib/python3.7/site-packages (from lightgbm) (1.7.1)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from lightgbm) (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn!=0.22.0->lightgbm) (3.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: PyArrow in /home/emr-notebook/.local/lib/python3.7/site-packages (6.0.0)\n",
      "Requirement already satisfied: numpy>=1.16.6 in /home/emr-notebook/.local/lib/python3.7/site-packages (from PyArrow) (1.21.5)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: scikit-plot in /home/emr-notebook/.local/lib/python3.7/site-packages (0.3.7)\n",
      "Requirement already satisfied: matplotlib>=1.4.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-plot) (3.4.3)\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-plot) (1.0.1)\n",
      "Requirement already satisfied: joblib>=0.10 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-plot) (1.1.0)\n",
      "Requirement already satisfied: scipy>=0.9 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-plot) (1.7.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/emr-notebook/.local/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /home/emr-notebook/.local/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /mnt/notebook-env/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/emr-notebook/.local/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /mnt/notebook-env/lib/python3.7/site-packages (from matplotlib>=1.4.0->scikit-plot) (2.4.7)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/notebook-env/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib>=1.4.0->scikit-plot) (1.15.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/emr-notebook/.local/lib/python3.7/site-packages (from scikit-learn>=0.18->scikit-plot) (3.0.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: cython in /home/emr-notebook/.local/lib/python3.7/site-packages (0.29.24)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://mirrors.ustc.edu.cn/pypi/web/simple\n",
      "Requirement already satisfied: pandas in /mnt/notebook-env/lib/python3.7/site-packages (1.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /mnt/notebook-env/lib/python3.7/site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /mnt/notebook-env/lib/python3.7/site-packages (from pandas) (2021.1)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/emr-notebook/.local/lib/python3.7/site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: six>=1.5 in /mnt/notebook-env/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!/emr/notebook-env/bin/pip3 install --upgrade -i https://mirrors.ustc.edu.cn/pypi/web/simple numpy\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple matplotlib\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple joblib\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple pyspark==3.1.1\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple sklearn\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple scikit-learn\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple lightgbm\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple PyArrow\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple scikit-plot\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple cython\n",
    "!/emr/notebook-env/bin/pip3 install -i https://mirrors.ustc.edu.cn/pypi/web/simple pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, random, math, os, time, glob\n",
    "from datetime import datetime, date, timedelta\n",
    "import pytz\n",
    "from dateutil.relativedelta import *\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import logging\n",
    "from collections import namedtuple, deque\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', 5000)\n",
    "pd.options.display.float_format = '{:,.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DateType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import when, row_number, expr, concat, format_number, avg, isnan, weekofyear, rand\n",
    "from pyspark.ml.feature import Bucketizer, VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression as ml_LinearRegression \n",
    "from pyspark.sql import DataFrameStatFunctions as statFunc\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import collect_set, col, count\n",
    "from pyspark.ml.fpm import FPGrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler as sk_StandardScaler\n",
    "from sklearn.decomposition import PCA as sk_pca\n",
    "from sklearn.pipeline import Pipeline as sk_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression as sk_LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import scikitplot as skplt\n",
    "from sklearn.preprocessing import OneHotEncoder as sk_OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from pyspark.ml import Pipeline as ml_pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StandardScaler as ml_StandardScaler\n",
    "from pyspark.ml.feature import PCA as ml_pca\n",
    "from pyspark.ml.classification import LogisticRegression as ml_LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "os.environ[\"SPARK_CONF_DIR\"] = \"/usr/lib/spark/conf\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/etc/hadoop/conf\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3\"\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/lib/spark\"\n",
    "\n",
    "#spark = SparkSession.builder.appName(\"test3\").getOrCreate()\\\n",
    "#spark = SparkSession.builder.appName(\"local[*]\").master(\"local[*]\")\\\n",
    "spark = SparkSession.builder.master(\"yarn\").appName(\"test\")\\\n",
    "    .config(\"spark.executor.instances\", \"12\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"8g\")\\\n",
    "    .config(\"spark.driver.memory\", \"100g\")\\\n",
    "    .enableHiveSupport().getOrCreate()\n",
    "#     .config(\"spark.jars.packages\", \"com.microsoft.ml.spark:mmlspark_2.11:1.0.0-rc1\")\\\n",
    "#     .config(\"spark.jars.repositories\", \"https://mmlspark.azureedge.net/maven\")\\\n",
    "#     # .config('spark.local.dir', 'SOME/DIR/WHERE/YOU/HAVE/SPACE')\n",
    "    # .config(\"spark.sql.parquet.writeLegacyFormat\", true)\\\n",
    "    #spark = SparkSession.builder.appName(\"test1\")\\\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.allowCreatingManagedTableUsingNhonemptyLocation\", \"true\")\n",
    "spark.conf.set(\"hive.exec.dynamic.partition\", \"true\");\n",
    "spark.conf.set(\"hive.exec.dynamic.partition.mode\", \"nonstrict\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc=spark.conf\n",
    "spark\n",
    "spark.sparkContext._conf.get('spark.driver.memory')\n",
    "spark.sparkContext._conf.get('spark.executor.memory')\n",
    "spark.sparkContext._conf.get('spark.num.executors')\n",
    "spark.sparkContext._conf.get('spark.executor.cores')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# spark.sql('drop database IF EXISTS dwd')\n",
    "# spark.sql('drop database IF EXISTS dim')\n",
    "spark.sql('drop table IF EXISTS dwd.dwd_sales_order_main_flow')\n",
    "spark.sql('drop table IF EXISTS dwd.dwd_sales_order_detail_flow')\n",
    "spark.sql('drop table IF EXISTS dwd.dwd_sales_order_return_main')\n",
    "spark.sql('drop table IF EXISTS dwd.dwd_sales_order_return_detail')\n",
    "spark.sql('drop table IF EXISTS dwd.dwd_event')\n",
    "spark.sql('drop table IF EXISTS dim.dim_agreement_coupon_info')\n",
    "spark.sql('drop table IF EXISTS dwd.dwd_agreement_coupon_ver')\n",
    "spark.sql('drop table IF EXISTS dim.dim_party_user_all')\n",
    "spark.sql('drop table IF EXISTS dim.dim_party_shop')\n",
    "spark.sql('drop table IF EXISTS dim.dim_item_sku')\n",
    "spark.sql('drop table IF EXISTS dim.dim_party_number_kid')\n",
    "spark.sql('drop table IF EXISTS dim.dim_public_date_calendar')\n",
    "spark.sql('drop table IF EXISTS dim.dim_public_location_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('create database IF NOT EXISTS dwd')\n",
    "spark.sql('create database IF NOT EXISTS dim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dwd`.`dwd_sales_order_main_flow`(`type_name` STRING,`type_value` STRING,`main_order_code` STRING, `source_channel` STRING, \\\n",
    "`refining_channel` STRING, `source_type` STRING, `pay_time` TIMESTAMP, `order_state` STRING, `order_state_note` STRING, \\\n",
    "`openid` STRING, `shopperid` STRING, `member_code` STRING, `user_phone_no` STRING, `user_phone_take_no` STRING,\\\n",
    "`consignee_info_code` STRING, `lego_shop_code` STRING, `dealers_shop_code` STRING, `order_order_state` STRING, \\\n",
    "`order_province` STRING, `order_city` STRING, `order_county` STRING, `order_address` STRING, `pay_polytype` STRING,\\\n",
    "`is_special_sales` STRING, `special_sales_type` STRING, `is_gwp` STRING, `receivable_amount` DOUBLE, `receipts_amount` DOUBLE, \\\n",
    "`sales_num` DOUBLE, `discount_amount` DOUBLE, `freight_amount` DOUBLE, `operator_code` STRING, `note` STRING, `create_time` TIMESTAMP, \\\n",
    "`update_time` TIMESTAMP, `achieve_time` TIMESTAMP, `delivery_time` TIMESTAMP, `scheduling_delivery_time` TIMESTAMP, \\\n",
    "`cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627545806',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dwd.db/dwd_sales_order_main_flow'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dwd`.`dwd_sales_order_detail_flow`(`type_name` STRING,`type_value` STRING,`source_channel` STRING, `refining_channel` STRING, \\\n",
    "`main_order_code` STRING, `detail_order_code` STRING, `source_type` STRING, `pay_time` TIMESTAMP, `order_state` STRING, \\\n",
    "`order_state_note` STRING, `openid` STRING, `opperid` STRING, `member_code` STRING, `mobile` STRING, `lego_shop_code` STRING,\\\n",
    "`dealers_shop_code` STRING, `item_five_lego_code` STRING, `sku_name` STRING, `is_gwp` STRING, `gwp_type` STRING, \\\n",
    "`item_unit_actual_amount` DOUBLE, `item_unit_standard_amount` DOUBLE, `receivable_amount` DOUBLE, `receipts_amount` DOUBLE, \\\n",
    "`sales_num` DOUBLE, `discount_amount` DOUBLE, `note` STRING, `create_time` TIMESTAMP, `update_time` TIMESTAMP,\\\n",
    "`completion_ime` TIMESTAMP, `delivery_time` TIMESTAMP, `advance_delivery_time` TIMESTAMP, `order_rrp_amount` DOUBLE, `member_detail_id` STRING, `cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627547702',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dwd.db/dwd_sales_order_detail_flow'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dwd`.`dwd_sales_order_return_main`(`type_name` STRING,`type_value` STRING,`return_order_code` STRING, `main_order_code` STRING, \\\n",
    "`source_channel` STRING, `refining_channel` STRING, `source_type` STRING, `pay_time` TIMESTAMP, `return_order_state` STRING,\\\n",
    "`return_order_type` STRING, `openid` STRING, `shopperid` STRING, `member_code` STRING, `mobile` STRING, `lego_shop_code` STRING,\\\n",
    "`lego_shop_name` STRING, `dealers_shop_code` STRING, `order_state` STRING, `order_province` STRING, `order_city` STRING,\\\n",
    "`order_county` STRING, `order_address` STRING, `receivable_amount` DOUBLE, `paid_in_amount` DOUBLE, `return_amount` DOUBLE,\\\n",
    "`return_num` BIGINT, `create_time` TIMESTAMP, `return_time` TIMESTAMP, `update_time` TIMESTAMP, `cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627547764',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dwd.db/dwd_sales_order_return_main'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dwd`.`dwd_sales_order_return_detail`(`type_name` STRING,`type_value` STRING,`return_ordera_detail_code` STRING, `detail_order_code` STRING, \\\n",
    "`main_order_code` STRING, `return_order_code` STRING, `source_channel` STRING, `refining_channel` STRING, `source_type` STRING, \\\n",
    "`pay_time` TIMESTAMP, `return_order_state` STRING, `return_order_type` STRING, `openid` STRING, `shopperid` STRING, `member_code` STRING, \\\n",
    "`mobile` STRING, `lego_shop_code` STRING, `lego_shop_name` STRING, `dealers_shop_code` STRING, `item_five_lego_code` STRING,\\\n",
    "`lego_sku_name` STRING, `is_gwp` STRING, `gwp_type` STRING, `receivable_amount` DOUBLE, `paid_in_amount` DOUBLE, `return_amount` DOUBLE,\\\n",
    "`return_num` BIGINT, `note` STRING, `return_time` TIMESTAMP, `create_time` TIMESTAMP, `update_time` TIMESTAMP, `cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627548594',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dwd.db/dwd_sales_order_return_detail'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dim`.`dim_agreement_coupon_info`(`coupon_code` STRING, `coupon_name` STRING, `coupon_type` STRING,\\\n",
    "`coupon_type_name` STRING, `mutex_other` STRING, `mutex_self` STRING, `coupon_priority` STRING, `reduce_condition_type` STRING,\\\n",
    "`reduce_condition_value` STRING, `reduce_action_type` STRING, `reduce_value` STRING, `max_reduce_value` STRING, `valid_start_type` STRING,\\\n",
    "`valid_delay_days` TIMESTAMP, `valid_start_time` TIMESTAMP, `valid_end_time` TIMESTAMP, `valid_time_length` DOUBLE, `valid_time_unit` STRING,\\\n",
    "`coupon_definition_status` STRING, `total_limit` DOUBLE, `sent_count` DOUBLE, `collected_count` DOUBLE, `redeemed_count` DOUBLE, \\\n",
    "`provide_type` STRING, `trigger_event` STRING, `max_collect_by_member` DOUBLE, `facevalue_per_unit` DOUBLE, `fairvalue_per_unit` DOUBLE, \\\n",
    "`cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627556713',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dim.db/dim_agreement_coupon_info'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dwd`.`dwd_agreement_coupon_ver`(`type_name` STRING,`type_value` STRING,`user_phone_no` STRING, `coupon_code` STRING, `crm_member_id` STRING,\\\n",
    "`crm_coupon_benefit_type` STRING, `crm_coupon_order_type` STRING, `benefit_id` STRING, `verification_id` STRING, `coupon_type` STRING, \\\n",
    "`order_shop` STRING, `coupon_scope` STRING, `coupon_scope_type` STRING, `channe_source` STRING, `channe_source_name` STRING,\\\n",
    "`source_campaign_code` STRING, `source_campaign_name` STRING, `order_time` TIMESTAMP, `order_num` BIGINT, `benefit_account_type_code` STRING,\\\n",
    "`benefit_account_type_name` STRING, `cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627557103',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dwd.db/dwd_agreement_coupon_ver'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dim`.`dim_party_shop`(`lego_shop_code` STRING, `lego_shop_name` STRING, `channel_source` STRING,\\\n",
    "`channel` STRING, `customer_code` STRING, `distributor_name`STRING, `lego_store_code` STRING, `dealer_name` STRING, \\\n",
    "`state_name` STRING, `region` STRING, `province_cn` STRING, `city_cn` STRING, `shop_address` STRING, `is_close` STRING,\\\n",
    "`open_date` DATE, `shop_area` STRING, `contact` STRING, `contact_phone_no` STRING, `area` DOUBLE, `zip_code` STRING,\\\n",
    "`first_sales_time` TIMESTAMP, `close_time` TIMESTAMP, `jurisdiction` STRING, `note` STRING, `up_to_amount` DOUBLE,\\\n",
    "`order_freight_amount_limit` DOUBLE, `partner_name` STRING, `sold_to` STRING, `ship_to` STRING, `trad_id` STRING, \\\n",
    "`review_channel_name` STRING, `sub_distributor_name` STRING, `sub_distributor_lvl1_name` STRING, `sub_distributor_lvl2_name` STRING, \\\n",
    "`sub_distributor_lvl3_name` STRING, `distributor_group` STRING, `distributor_group_name` STRING, `cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627551496',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dim.db/dim_party_shop'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dim`.`dim_item_sku`(`year_version` STRING, `lego_sku_id` STRING, `material_id` STRING, \\\n",
    "`sku_name` STRING, `sku_price_code` STRING, `rsp` DOUBLE, `new_sku_start_time` TIMESTAMP, `new_sku_end_time` TIMESTAMP,\\\n",
    "`sku_version_id` STRING, `version_num` STRING, `sku_specifications_id` STRING, `sku_specifications_name` STRING, \\\n",
    "`product_reveal_date` TIMESTAMP, `product_status` STRING, `sku_name_en` STRING, `sku_name_cn` STRING, `global_launch_date` TIMESTAMP, \\\n",
    "`global_delete_date` TIMESTAMP, `is_bu_cn_launch` STRING, `bu_cn_launch_date` TIMESTAMP, `bu_cn_delete_date` TIMESTAMP, \\\n",
    "`cn_lcs_launch_date` TIMESTAMP, `cn_ldc_launch_date` TIMESTAMP, `cn_tm_launch_date` TIMESTAMP, `cn_jd_b2b_launch_date` TIMESTAMP,\\\n",
    "`cn_jd_pop_launch_date` TIMESTAMP, `age_mark` STRING, `case_pack` STRING, `sku_unit` STRING, `lbs_size` DOUBLE, `sku_weight` DOUBLE, \\\n",
    "`sku_length` DOUBLE, `sku_width` DOUBLE, `sku_height` DOUBLE, `member_price` DOUBLE, `cost_price` DOUBLE, `stock_price` DOUBLE, \\\n",
    "`pricing_manner` STRING, `topic` STRING, `topic_name` STRING, `topic_group` STRING, `top_theme` STRING, `top_theme_name` STRING,\\\n",
    "`core_line` STRING, `is_bu_d2c_launch` STRING, `bu_d2c_launch_date` TIMESTAMP, `bu_d2c_delete_date` TIMESTAMP,\\\n",
    "`exclusive_products` STRING, `super_segment` STRING, `item_cn_line` STRING, `product_class` STRING, `is_batteries_included` STRING,\\\n",
    "`piece_count` DOUBLE, `product_ean_code` STRING, `carton_ean_code` STRING, `new_List_price_pricelist_ab_china` STRING,\\\n",
    "`new_ldc_extended_line_list_price` STRING, `material_group_name` STRING, `material_group` STRING, `product_sales_category` STRING,\\\n",
    "`sub_catergory` STRING, `fr_theme` STRING, `prod_category` STRING, `cn_tm_product_substatus` STRING, `cn_jd_b2b_product_substatus` STRING,\\\n",
    "`cn_jd_pop_product_substatus` STRING, `cn_jd_substatus` STRING, `in_and_out` STRING, `product_bu` STRING, `cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627556215',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dim.db/dim_item_sku'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dim`.`dim_party_number_kid`(`user_phone_no` STRING, `crm_member_id` STRING, `kid_id` STRING,\\\n",
    "`kid_name` STRING, `kid_idno` STRING, `kid_birthday` DATE, `cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627553796',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dim.db/dim_party_number_kid'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dim`.`dim_public_date_calendar`(`calendar_id` STRING, `date_name` STRING, `lunar_date_name` STRING,\\\n",
    "`year` STRING, `month` STRING, `year_month` STRING, `quarter` STRING, `quarter_name` STRING, `month_fist_calendar_id` STRING, \\\n",
    "`month_fist_day` STRING, `month_last_calendar_id` STRING, `month_last_day` STRING, `days_in_month` BIGINT, `weeks` BIGINT, \\\n",
    "`last_week_days` STRING, `last_year_days` STRING, `week_first_days` STRING, `week_last_days` STRING, `weeks_num` BIGINT,\\\n",
    "`weeks_name` STRING, `year_day_num` BIGINT, `year_day_weeks` BIGINT, `holiday_flag` STRING, `half_year` STRING, `if_month_lastday` STRING, \\\n",
    "`calendar_holiday_name` STRING, `lunar_holiday_name` STRING, `constellation` STRING, `chinese_zodiac` STRING, `lunar_year` STRING, \\\n",
    "`lunar_month` STRING, `lunar_month_run` STRING, `lunar_day` STRING, `solar_term_name` STRING, `lego_day` BIGINT, `lego_week` BIGINT, \\\n",
    "`lego_month` BIGINT, `lego_year` BIGINT, `cdp_modify_time` TIMESTAMP, `pt` STRING)\\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627553826',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dim.db/dim_public_date_calendar'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dim`.`dim_public_location_area`(`area_code` STRING, `province` STRING, `city` STRING, `telecom_operator` STRING) \\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627557721',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dim.db/dim_public_location_area'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dim`.`dim_party_user_all`(`register_phone_no` STRING,\\\n",
    "`is_member` STRING, `is_shopper` STRING, `is_consumber` STRING, `member_card` STRING, `off_member_id` STRING,\\\n",
    "`crm_member_id` STRING, `wechat_app_openid` STRING, `tmall_member_id` STRING, `jd_member_id` STRING,\\\n",
    "`jd_shopperid` STRING, `tmall_shopperid` STRING, `wechat_unionid` STRING, `wechat_app_nickname` STRING,\\\n",
    "`tmall_nickname` STRING, `jd_nickname` STRING, `wechat_app_user_name` STRING, `tmall_user_name` STRING,\\\n",
    "`jd_user_name` STRING, `off_user_name` STRING, `gender` STRING, `birthday` DATE, `age_cohorts` STRING,\\\n",
    "`native_place` STRING, `family_income` BIGINT, `age` BIGINT, `state` STRING, `province` STRING, \\\n",
    "`city` STRING, `city_grade` STRING, `county` STRING, `street` STRING, `delivery_address` STRING,\\\n",
    "`marital_status` STRING, `chinese_zodiac` STRING, `constellation` STRING, `is_kid` STRING,\\\n",
    "`kid_num` BIGINT, `kid_gender_1` STRING, `kid_birthday_1` DATE, `kid_age_1` BIGINT, \\\n",
    "`kid_age_cohorts_1` BIGINT, `kid_age_learn_phase_1` BIGINT, `kid_gender_2` STRING, `kid_birthday_2` DATE, \\\n",
    "`kid_age_2` BIGINT, `kid_age_cohorts_2` BIGINT, `kid_age_learn_phase_2` BIGINT, `kid_gender_3` STRING, \\\n",
    "`kid_birthday_3` DATE, `kid_age_3` BIGINT, `kid_age_cohorts_3` BIGINT, `kid_age_learn_phase_3` BIGINT, \\\n",
    "`is_consent_letter` STRING, `is_touch_crm` STRING, `first_buy_date` DATE, `all_channel_first_day_num_today` BIGINT,\\\n",
    "`all_channel_first_day_num_crm` BIGINT, `last_buy_date` DATE, `all_channel_last_day_num_pos` BIGINT, \\\n",
    "`all_channel_his_buy_avg_day_num` BIGINT, `crm_usable_coupon` BIGINT, `coupon_preference_verification_m` BIGINT,\\\n",
    "`his_add_convert_point_preference_m` BIGINT, `about_to_past_due_point_num_Xm` BIGINT, \\\n",
    "`wechat_app_member_grade` STRING,`pos_member_grade` STRING, `jd_member_grade` STRING, \\\n",
    "`tmall_member_grade` STRING, `is_wechat_app_member` STRING, `wechat_app_member_register_date` DATE,\\\n",
    "`wechat_app_member_register_age_month` BIGINT, `is_tmall_member` STRING, `tmall_member_register_date` DATE,\\\n",
    "`tmall_member_register_age_month` BIGINT, `tmall_buy_first_time` DATE, `tmall_buy_last_time` DATE,\\\n",
    "`tmall_buy_last_register_day_num` BIGINT, `tmall_buy_first_register_day_num` BIGINT, \\\n",
    "`tmall_his_buy_avg_day_num` BIGINT, `tmall_buy_first_day_num_today` BIGINT, `tmall_buy_last_day_num_today` BIGINT,\\\n",
    "`is_jd_member` STRING, `jd_member_register_date` DATE, `jd_member_register_age_month` BIGINT, \\\n",
    "`jd_buy_first_time` DATE, `jd_buy_last_time` DATE, `jd_buy_last_register_day_num` BIGINT, \\\n",
    "`jd_buy_first_register_day_num` BIGINT, `jd_his_buy_avg_day_num` BIGINT, `jd_buy_first_day_num_today` BIGINT,\\\n",
    "`jd_buy_last_day_num_today` BIGINT, `is_crm_member` STRING, `crm_member_starts` STRING, \\\n",
    "`crm_member_register_date` DATE, `crm_member_register_age_month` BIGINT, `is_inside_staff` STRING, \\\n",
    "`is_blacklist` STRING, `is_dealer_member` STRING, `pos_memeber_register_date` DATE,\\\n",
    "`pos_member_register_age_month` BIGINT, `pos_buy_first_time` DATE, `pos_buy_last_time` DATE, \\\n",
    "`pos_buy_last_register_day_num` BIGINT, `pos_buy_first_register_day_num` BIGINT, \\\n",
    "`pos_his_buy_avg_day_num` BIGINT, `pos_buy_first_day_num_today` BIGINT, `pos_buy_last_day_num_today` BIGINT,\\\n",
    "`crm_identity_type` STRING, `crm_identity_code` STRING, `crm_usable_point` BIGINT,\\\n",
    "`user_email` STRING, `zip_code` STRING, `educational_status` STRING, `user_job` STRING, \\\n",
    "`monthly_pay` BIGINT, `hobbies_interests` STRING, `member_grade_code` STRING, `member_grade_name` STRING, \\\n",
    "`ly_associated_rate` STRING, `tm_associated_rate` STRING, `jd_associated_rate` STRING, \\\n",
    "`his_past_due_all_point` BIGINT, `all_buy_packages_num` BIGINT, `all_buy_order_num` BIGINT, \\\n",
    "`all_buy_associated_rate` STRING, `is_weekend_user` STRING, `is_festival_user` STRING,\\\n",
    "`is_cny_festival_user` STRING, `is_kid_festival_user` STRING, `is_holiday_festival_user` STRING, \\\n",
    "`is_christmas_festival_user` STRING, `is_festival_user_618` STRING, `is_festival_user_1111` STRING,\\\n",
    "`is_festival_user_1212` STRING, `is_super_brand_day` STRING,\\\n",
    "`crm_member_channel` STRING ,\\\n",
    "`is_dealer_member_s` STRING ,\\\n",
    "`dealer_member_source` STRING,\\\n",
    "`crm_thirdparty_channel` STRING ,\\\n",
    "`transaction_six_months` STRING,\\\n",
    "`tmall_fst_hyt_purchase` STRING ,\\\n",
    "`tmall_fst_binding_time` STRING ,\\\n",
    "`tmall_buy_purchase_cnt` BIGINT,\\\n",
    "`lewin_member_degree` STRING,\\\n",
    "`channel_name` STRING,\\\n",
    "`org_name` STRING,\\\n",
    "`member_channel_name` STRING,\\\n",
    "`member_org_name` STRING,`tmall_ouid` STRING,\\\n",
    "`cdp_modify_time` TIMESTAMP, `pt` STRING) \\\n",
    "PARTITIONED BY (`dt` STRING) \\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde' \\\n",
    "WITH SERDEPROPERTIES ( \\\n",
    "  'serialization.format' = '1' \\\n",
    ") \\\n",
    "STORED AS \\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat' \\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat' \\\n",
    "TBLPROPERTIES ( \\\n",
    "  'transient_lastDdlTime' = '1627550341', \\\n",
    "  'orc.compress' = 'NONE' \\\n",
    ") \\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dim.db/dim_party_user_all'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE external TABLE IF NOT EXISTS `dwd`.`dwd_event`(`eventdate` STRING, `user` STRING, `event_key` STRING, `freq` STRING, `pt` STRING) \\\n",
    "PARTITIONED BY (`dt` STRING)\\\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.orc.OrcSerde'\\\n",
    "WITH SERDEPROPERTIES (\\\n",
    "  'serialization.format' = '1'\\\n",
    ")\\\n",
    "STORED AS\\\n",
    "  INPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcInputFormat'\\\n",
    "  OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat'\\\n",
    "TBLPROPERTIES (\\\n",
    "  'transient_lastDdlTime' = '1627557721',\\\n",
    "  'orc.compress' = 'NONE'\\\n",
    ")\\\n",
    "location 'hdfs://172.20.37.218:8020/user/hive/warehouse/dwd.db/dwd_event'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('msck repair table dwd.dwd_sales_order_main_flow')\n",
    "spark.sql('msck repair table dwd.dwd_sales_order_detail_flow')\n",
    "spark.sql('msck repair table dwd.dwd_sales_order_return_main')\n",
    "spark.sql('msck repair table dwd.dwd_sales_order_return_detail')\n",
    "spark.sql('msck repair table dwd.dwd_event')\n",
    "spark.sql('msck repair table dim.dim_agreement_coupon_info')\n",
    "spark.sql('msck repair table dwd.dwd_agreement_coupon_ver')\n",
    "spark.sql('msck repair table dim.dim_party_user_all')\n",
    "spark.sql('msck repair table dim.dim_party_shop')\n",
    "spark.sql('msck repair table dim.dim_item_sku')\n",
    "spark.sql('msck repair table dim.dim_party_number_kid')\n",
    "spark.sql('msck repair table dim.dim_public_date_calendar')\n",
    "spark.sql('msck repair table dim.dim_public_location_area')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|     dwd|dwd_agreement_cou...|      false|\n",
      "|     dwd|           dwd_event|      false|\n",
      "|     dwd|dwd_sales_order_d...|      false|\n",
      "|     dwd|dwd_sales_order_m...|      false|\n",
      "|     dwd|dwd_sales_order_r...|      false|\n",
      "|     dwd|dwd_sales_order_r...|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n",
      "+--------+--------------------+-----------+\n",
      "|database|           tableName|isTemporary|\n",
      "+--------+--------------------+-----------+\n",
      "|     dim|dim_agreement_cou...|      false|\n",
      "|     dim|        dim_item_sku|      false|\n",
      "|     dim|dim_party_number_kid|      false|\n",
      "|     dim|      dim_party_shop|      false|\n",
      "|     dim|  dim_party_user_all|      false|\n",
      "|     dim|dim_public_date_c...|      false|\n",
      "|     dim|dim_public_locati...|      false|\n",
      "+--------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('show tables in dwd').show()\n",
    "spark.sql('show tables in dim').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
